{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will classify stimulus classes using the Haxby et al. data. You should first obtain the data using the command:\n",
    "\n",
    "wget http://data.pymvpa.org/datasets/haxby2001/subj1-2010.01.14.tar.gz\n",
    "\n",
    "and set the datadir variable accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from /Users/poldrack/data_unsynced/haxby/subj1\n"
     ]
    }
   ],
   "source": [
    "import nipype.algorithms.modelgen as model   # model generation\n",
    "import nipype.interfaces.fsl as fsl          # fsl\n",
    "from nipype.interfaces.base import Bunch\n",
    "import os,json,glob\n",
    "import numpy\n",
    "import nibabel\n",
    "import nilearn.plotting\n",
    "import sklearn.multiclass\n",
    "from sklearn.svm import LinearSVC\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import scipy.stats\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from haxby_data import HaxbyData\n",
    "\n",
    "\n",
    "datadir='/Users/poldrack/data_unsynced/haxby/subj1'\n",
    "\n",
    "print 'Using data from',datadir\n",
    "\n",
    "haxbydata=HaxbyData(datadir)\n",
    "\n",
    "boldimg=nibabel.load(haxbydata.boldfile)\n",
    "\n",
    "if not os.path.exists(haxbydata.brainmaskfile):\n",
    "    bet=fsl.BET()\n",
    "    bet.inputs.in_file=haxbydata.boldfile\n",
    "    bet.inputs.out_file=haxbydata.boldfile.replace('.nii.gz','_brain.nii.gz')\n",
    "    bet.inputs.functional=True\n",
    "    bet.inputs.mask=True\n",
    "    bet.run()\n",
    "\n",
    "\n",
    "brainmaskimg=nibabel.load(haxbydata.brainmaskfile)\n",
    "vtmaskimg=nibabel.load(haxbydata.vtmaskfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modeldir=os.path.join(datadir,'blockmodel')\n",
    "# no way to specify the output directory, so we just chdir into the \n",
    "# desired output directory\n",
    "if not os.path.exists(modeldir):\n",
    "    os.mkdir(modeldir)\n",
    "os.chdir(modeldir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the model with a separate condition for each block using FSL.  This will take several hours to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stats have already been run - using existing files\n"
     ]
    }
   ],
   "source": [
    "contrasts=[]\n",
    "\n",
    "for i in range(len(haxbydata.conditions)):\n",
    "    contrasts.append([haxbydata.conditions[i],'T',[haxbydata.conditions[i]],[1]])\n",
    "\n",
    "\n",
    "# this is how one could do it using FSL - this is VERY slow, so let's compute the GLM on our own\n",
    "if not os.path.exists(os.path.join(modeldir,'stats')):\n",
    "    \n",
    "    \n",
    "    info = [Bunch(conditions=haxbydata.conditions,\n",
    "                  onsets=haxbydata.onsets,\n",
    "                  durations=haxbydata.durations)\n",
    "           ]\n",
    "    print 'SpecifyModel'\n",
    "    s = model.SpecifyModel()\n",
    "    s.inputs.input_units = 'secs'\n",
    "    s.inputs.functional_runs = [haxbydata.boldbrainfile]\n",
    "    s.inputs.time_repetition = haxbydata.tr\n",
    "    s.inputs.high_pass_filter_cutoff = 128.\n",
    "    s.inputs.subject_info = info\n",
    "    s.run()\n",
    "\n",
    "    print 'level1design'\n",
    "    level1design = fsl.model.Level1Design()\n",
    "    level1design.inputs.interscan_interval = haxbydata.tr\n",
    "    level1design.inputs.bases = {'dgamma':{'derivs': False}}\n",
    "    level1design.inputs.session_info = s._sessinfo\n",
    "    level1design.inputs.model_serial_correlations=False\n",
    "    level1design.inputs.contrasts=contrasts\n",
    "    level1info=level1design.run() \n",
    "    \n",
    "    fsf_file=os.path.join(modeldir,'run0.fsf')\n",
    "    matfile=fsf_file.replace(\".fsf\",\".mat\")\n",
    "    event_files=glob.glob(os.path.join(modeldir,'ev*txt'))\n",
    "\n",
    "    print 'modelgen'\n",
    "    modelgen=fsl.model.FEATModel()\n",
    "    modelgen.inputs.fsf_file=fsf_file\n",
    "    modelgen.inputs.ev_files=event_files\n",
    "    modelgen.run()\n",
    "\n",
    "    print 'FILMGLS'\n",
    "    fgls = fsl.FILMGLS(autocorr_noestimate=True)\n",
    "    fgls.inputs.in_file =haxbydata.boldbrainfile\n",
    "    fgls.inputs.design_file = os.path.join(modeldir,'run0.mat')\n",
    "    fgls.inputs.threshold = 10\n",
    "    fgls.inputs.results_dir = os.path.join(modeldir,'stats')\n",
    "    fgls.inputs.tcon_file=os.path.join(modeldir,'run0.con')\n",
    "    res = fgls.run() \n",
    "\n",
    "else:\n",
    "    print 'stats have already been run - using existing files'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the zstat images that we will use as our block-by-block signal estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_whole_brain=False\n",
    "\n",
    "if not os.path.exists(os.path.join(modeldir,'zstatdata.nii.gz')):\n",
    "    zstatdata=numpy.zeros((boldimg.shape[0],boldimg.shape[1],boldimg.shape[2],len(haxbydata.conditions)))\n",
    "    for i in range(len(conditions)):\n",
    "        zstatdata[:,:,:,i]=nibabel.load(os.path.join(modeldir,'stats/zstat%d.nii.gz'%int(i+1))).get_data()\n",
    "\n",
    "    zstatimg=nibabel.Nifti1Image(zstatdata,affine=brainmaskimg.get_affine())\n",
    "    zstatimg.to_filename(os.path.join(modeldir,'zstatdata.nii.gz'))\n",
    "\n",
    "if use_whole_brain:\n",
    "    maskimg=haxbydata.brainmaskfile\n",
    "else:\n",
    "    maskimg=haxbydata.vtmaskfile\n",
    "    \n",
    "nifti_masker = NiftiMasker(mask_img=maskimg, standardize=False)\n",
    "fmri_masked = nifti_masker.fit_transform(os.path.join(modeldir,'zstatdata.nii.gz'))\n",
    "\n",
    "# include faces and cats\n",
    "condition_mask = numpy.logical_or(haxbydata.condnums == 2,\n",
    "                               haxbydata.condnums == 3)\n",
    "fmri_masked = fmri_masked[condition_mask,:]\n",
    "condlabels=haxbydata.condnums[condition_mask]\n",
    "runlabels=haxbydata.runs[condition_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a leave-one-run out classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_within_runs(labels,runs):\n",
    "    for r in numpy.unique(runs):\n",
    "        l=labels[runs==r]\n",
    "        random.shuffle(l)\n",
    "        labels[runs==r]=l\n",
    "    return labels\n",
    "\n",
    "\n",
    "def run_classifier(fmri_masked,condlabels,runs,baseclf,shuffle_labels=False):\n",
    "    cv = sklearn.cross_validation.LeaveOneLabelOut(labels=runs)\n",
    "\n",
    "    pred=numpy.zeros(len(runs)) # predicted class\n",
    "\n",
    "    if len(numpy.unique(condlabels))>2:\n",
    "        clf=sklearn.multiclass.OneVsRestClassifier(baseclf)\n",
    "    else:\n",
    "        clf=baseclf\n",
    "    \n",
    "    for train,test in cv:\n",
    "        testdata=fmri_masked[test,:]\n",
    "        traindata=fmri_masked[train,:]\n",
    "        trainlabels=condlabels[train]\n",
    "        if shuffle_labels:\n",
    "            shuffle_within_runs(trainlabels,runs[train])\n",
    "        clf.fit(traindata,trainlabels)\n",
    "        pred[test]=clf.predict(testdata)\n",
    "        \n",
    "    confmtx=sklearn.metrics.confusion_matrix(condlabels,pred)\n",
    "    acc=sklearn.metrics.accuracy_score(condlabels,pred)\n",
    "    return pred,confmtx,acc\n",
    "\n",
    "pred,confmtx,acc=run_classifier(fmri_masked,condlabels,runlabels,LinearSVC())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 3]\n",
      " [4 8]]\n",
      "Accuracy score: 0.708333333333\n"
     ]
    }
   ],
   "source": [
    "print confmtx\n",
    "print 'Accuracy score:',acc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Run the classifier repeatedly using random labels to get a null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nperms=500\n",
    "randacc=numpy.zeros(nperms)\n",
    "for i in range(nperms):\n",
    "    p,c,randacc[i]=run_classifier(fmri_masked,haxbydata.condlabels,runlabels,LinearSVC(),shuffle_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pct=scipy.stats.percentileofscore(randacc,acc)\n",
    "print 'Pval:',(100-pct)/100.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up a searchlight analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nilearn.image import index_img\n",
    "fmri_img = index_img(zstatimg, condition_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fmri_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
