{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will classify stimulus classes using the Haxby et al. data. You should first obtain the data using the command:\n",
    "\n",
    "wget http://data.pymvpa.org/datasets/haxby2001/subj1-2010.01.14.tar.gz\n",
    "\n",
    "and set the datadir variable accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data from /Users/poldrack/data_unsynced/haxby/subj1\n"
     ]
    }
   ],
   "source": [
    "import nipype.algorithms.modelgen as model   # model generation\n",
    "import nipype.interfaces.fsl as fsl          # fsl\n",
    "from nipype.interfaces.base import Bunch\n",
    "import os,json,glob\n",
    "import numpy\n",
    "import nibabel\n",
    "import nilearn.plotting\n",
    "import sklearn.multiclass\n",
    "from sklearn.svm import LinearSVC\n",
    "import sklearn.metrics\n",
    "import sklearn.cross_validation\n",
    "import statsmodels.api as sm\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import scipy.stats\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "datadir='/Users/poldrack/data_unsynced/haxby/subj1'\n",
    "\n",
    "print 'Using data from',datadir\n",
    "\n",
    "\n",
    "tr=2.5\n",
    "\n",
    "boldfile=os.path.join(datadir,'bold.nii.gz')\n",
    "boldbrainfile=os.path.join(datadir,'bold_brain.nii.gz')\n",
    "vtmaskfile=os.path.join(datadir,'mask4_vt.nii.gz')\n",
    "brainmaskfile=os.path.join(datadir,'bold_brain_mask.nii.gz')\n",
    "\n",
    "boldimg=nibabel.load(boldfile)\n",
    "\n",
    "if not os.path.exists(brainmaskfile):\n",
    "    bet=fsl.BET()\n",
    "    bet.inputs.in_file=boldfile\n",
    "    bet.inputs.out_file=boldfile.replace('.nii.gz','_brain.nii.gz')\n",
    "    bet.inputs.functional=True\n",
    "    bet.inputs.mask=True\n",
    "    bet.run()\n",
    "\n",
    "\n",
    "brainmaskimg=nibabel.load(brainmaskfile)\n",
    "vtmaskimg=nibabel.load(vtmaskfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelfile=os.path.join(datadir,'labels.txt')\n",
    "lines=open(labelfile).readlines()\n",
    "lines=lines[1:] # drop header\n",
    "\n",
    "# find all block onsets\n",
    "conditions=[]\n",
    "condnums=[]\n",
    "onsets=[]\n",
    "durations=[]\n",
    "runs=[]\n",
    "cond=''\n",
    "condctr=1\n",
    "\n",
    "cond_dict={'scissors':1,\n",
    " 'face':2,\n",
    " 'cat':3,\n",
    " 'shoe':4,\n",
    " 'house':5,\n",
    " 'scrambledpix':6,\n",
    " 'bottle':7,\n",
    " 'chair':8}\n",
    "condlabels=['scissors','face','cat','shoe','house','scrambledpix','bottle','chair']\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    l_s=lines[i].strip().split()\n",
    "    if l_s[0]=='rest':\n",
    "        continue\n",
    "    if not l_s[0]==cond:\n",
    "        cond=l_s[0]\n",
    "        runs.append(int(l_s[1]))\n",
    "        conditions.append('-'.join(l_s))\n",
    "        condnums.append(cond_dict[l_s[0]])\n",
    "        onsets.append([tr*(i+1)])\n",
    "        durations.append([22.5])\n",
    "    \n",
    "condnums=numpy.array(condnums)\n",
    "runs=numpy.array(runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modeldir=os.path.join(datadir,'blockmodel')\n",
    "# no way to specify the output directory, so we just chdir into the \n",
    "# desired output directory\n",
    "if not os.path.exists(modeldir):\n",
    "    os.mkdir(modeldir)\n",
    "os.chdir(modeldir)\n",
    "\n",
    "matfile=fsf_file.replace(\".fsf\",\".mat\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the model with a separate condition for each block using FSL.  This will take several hours to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpecifyModel\n",
      "level1design\n",
      "modelgen\n",
      "FILMGLS"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:interface:stdout 2015-07-16T16:45:35.737367:Log directory is: /Users/poldrack/data_unsynced/haxby/subj1/blockmodel/stats\n",
      "INFO:interface:stdout 2015-07-16T16:45:56.452997:paradigm.getDesignMatrix().Nrows()=1452\n",
      "INFO:interface:stdout 2015-07-16T16:45:56.453614:paradigm.getDesignMatrix().Ncols()=96\n",
      "INFO:interface:stdout 2015-07-16T16:45:56.453614:sizeTS=1452\n",
      "INFO:interface:stdout 2015-07-16T16:45:56.453614:numTS=42228\n",
      "INFO:interface:stdout 2015-07-16T16:45:56.757459:Completed\n",
      "INFO:interface:stdout 2015-07-16T16:45:56.757459:Prewhitening and Computing PEs...\n",
      "INFO:interface:stdout 2015-07-16T16:45:56.757459:Percentage done:\n",
      "INFO:interface:stdout 2015-07-16T19:20:00.605038:1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,Completed\n",
      "INFO:interface:stdout 2015-07-16T19:20:00.605038:Saving results... \n",
      "INFO:interface:stdout 2015-07-16T19:21:54.695397:Completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contrasts=[]\n",
    "\n",
    "for i in range(len(conditions)):\n",
    "    contrasts.append([conditions[i],'T',[conditions[i]],[1]])\n",
    "\n",
    "\n",
    "# this is how one could do it using FSL - this is VERY slow, so let's compute the GLM on our own\n",
    "if not os.path.exists(os.path.join(modeldir,'stats')):\n",
    "    \n",
    "    \n",
    "    info = [Bunch(conditions=conditions,\n",
    "                  onsets=onsets,\n",
    "                  durations=durations)\n",
    "           ]\n",
    "    print 'SpecifyModel'\n",
    "    s = model.SpecifyModel()\n",
    "    s.inputs.input_units = 'secs'\n",
    "    s.inputs.functional_runs = [boldbrainfile]\n",
    "    s.inputs.time_repetition = tr\n",
    "    s.inputs.high_pass_filter_cutoff = 128.\n",
    "    s.inputs.subject_info = info\n",
    "    s.run()\n",
    "\n",
    "    print 'level1design'\n",
    "    level1design = fsl.model.Level1Design()\n",
    "    level1design.inputs.interscan_interval = tr\n",
    "    level1design.inputs.bases = {'dgamma':{'derivs': False}}\n",
    "    level1design.inputs.session_info = s._sessinfo\n",
    "    level1design.inputs.model_serial_correlations=False\n",
    "    level1design.inputs.contrasts=contrasts\n",
    "    level1info=level1design.run() \n",
    "    \n",
    "    fsf_file=os.path.join(modeldir,'run0.fsf')\n",
    "    event_files=glob.glob(os.path.join(modeldir,'ev*txt'))\n",
    "\n",
    "    print 'modelgen'\n",
    "    modelgen=fsl.model.FEATModel()\n",
    "    modelgen.inputs.fsf_file=fsf_file\n",
    "    modelgen.inputs.ev_files=event_files\n",
    "    modelgen.run()\n",
    "\n",
    "    print 'FILMGLS'\n",
    "    fgls = fsl.FILMGLS(autocorr_noestimate=True)\n",
    "    fgls.inputs.in_file =boldbrainfile\n",
    "    fgls.inputs.design_file = os.path.join(modeldir,'run0.mat')\n",
    "    fgls.inputs.threshold = 10\n",
    "    fgls.inputs.results_dir = os.path.join(modeldir,'stats')\n",
    "    fgls.inputs.tcon_file=os.path.join(modeldir,'run0.con')\n",
    "    res = fgls.run() \n",
    "\n",
    "else:\n",
    "    print 'stats have already been run - using existing files'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the zstat images that we will use as our block-by-block signal estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_whole_brain=False\n",
    "\n",
    "if not os.path.exists(os.path.join(modeldir,'zstatdata.nii.gz')):\n",
    "    zstatdata=numpy.zeros((boldimg.shape[0],boldimg.shape[1],boldimg.shape[2],len(conditions)))\n",
    "    for i in range(len(conditions)):\n",
    "        zstatdata[:,:,:,i]=nibabel.load(os.path.join(modeldir,'stats/zstat%d.nii.gz'%int(i+1))).get_data()\n",
    "\n",
    "    zstatimg=nibabel.Nifti1Image(zstatdata,affine=brainmaskimg.get_affine())\n",
    "    zstatimg.to_filename(os.path.join(modeldir,'zstatdata.nii.gz'))\n",
    "\n",
    "if use_whole_brain:\n",
    "    maskimg=brainmaskfile\n",
    "else:\n",
    "    maskimg=vtmaskfile\n",
    "    \n",
    "nifti_masker = NiftiMasker(mask_img=maskimg, standardize=False)\n",
    "fmri_masked = nifti_masker.fit_transform(os.path.join(modeldir,'zstatdata.nii.gz'))\n",
    "\n",
    "# include faces and cats\n",
    "condition_mask = numpy.logical_or(condnums == 2,\n",
    "                               condnums == 3)\n",
    "fmri_masked = fmri_masked[condition_mask,:]\n",
    "condlabels=condnums[condition_mask]\n",
    "runlabels=runs[condition_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 3, 2, 2,\n",
       "       3])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a leave-one-run out classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shuffle_within_runs(labels,runs):\n",
    "    for r in numpy.unique(runs):\n",
    "        l=labels[runs==r]\n",
    "        random.shuffle(l)\n",
    "        labels[runs==r]=l\n",
    "    return labels\n",
    "\n",
    "\n",
    "def run_classifier(fmri_masked,condlabels,runs,baseclf,shuffle_labels=False):\n",
    "    cv = sklearn.cross_validation.LeaveOneLabelOut(labels=runs)\n",
    "\n",
    "    pred=numpy.zeros(len(runs)) # predicted class\n",
    "\n",
    "    if len(numpy.unique(condlabels))>2:\n",
    "        clf=sklearn.multiclass.OneVsRestClassifier(baseclf)\n",
    "    else:\n",
    "        clf=baseclf\n",
    "    \n",
    "    for train,test in cv:\n",
    "        testdata=fmri_masked[test,:]\n",
    "        traindata=fmri_masked[train,:]\n",
    "        trainlabels=condlabels[train]\n",
    "        if shuffle_labels:\n",
    "            shuffle_within_runs(trainlabels,runs[train])\n",
    "        clf.fit(traindata,trainlabels)\n",
    "        pred[test]=clf.predict(testdata)\n",
    "        \n",
    "    confmtx=sklearn.metrics.confusion_matrix(condlabels,pred)\n",
    "    acc=sklearn.metrics.accuracy_score(condlabels,pred)\n",
    "    return pred,confmtx,acc\n",
    "\n",
    "pred,confmtx,acc=run_classifier(fmri_masked,condlabels,runlabels,LinearSVC())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 3]\n",
      " [4 8]]\n",
      "Accuracy score: 0.708333333333\n"
     ]
    }
   ],
   "source": [
    "print confmtx\n",
    "print 'Accuracy score:',acc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Run the classifier repeatedly using random labels to get a null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nperms=500\n",
    "randacc=numpy.zeros(nperms)\n",
    "for i in range(nperms):\n",
    "    p,c,randacc[i]=run_classifier(fmri_masked,condlabels,runlabels,LinearSVC(),shuffle_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pval: 0.013\n"
     ]
    }
   ],
   "source": [
    "pct=scipy.stats.percentileofscore(randacc,acc)\n",
    "print 'Pval:',(100-pct)/100.0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
