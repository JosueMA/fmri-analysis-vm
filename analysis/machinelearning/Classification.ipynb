{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "import sklearn.neighbors\n",
    "import sklearn.cross_validation\n",
    "import sklearn.metrics\n",
    "import sklearn.lda\n",
    "import sklearn.svm\n",
    "import sklearn.linear_model\n",
    "\n",
    "n=100\n",
    "\n",
    "def make_class_data(mean=[50,110],multiplier=1.5,var=[[10,10],[10,10]],cor=-0.4,N=100):\n",
    "    \"\"\"\n",
    "    generate a synthetic classification data set with two variables\n",
    "    \"\"\"\n",
    "    cor=numpy.array([[1.,cor],[cor,1.]])\n",
    "    var1=numpy.array([[var[0][0],0],[0,var[0][1]]])\n",
    "    cov1=var1.dot(cor).dot(var1)\n",
    "    d1=numpy.random.multivariate_normal(mean,cov1,N/2)\n",
    "    var2=numpy.array([[var[1][0],0],[0,var[1][1]]])\n",
    "    cov2=var2.dot(cor).dot(var2)\n",
    "    d2=numpy.random.multivariate_normal(numpy.array(mean)*multiplier,cov2,N/2)\n",
    "    d=numpy.vstack((d1,d2))\n",
    "    cl=numpy.zeros(N)\n",
    "    cl[:(N/2)]=1\n",
    "    return cl,d"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl,d=make_class_data(multiplier=[1.1,1.1],N=n)\n",
    "print numpy.mean(d[:50,:],0)\n",
    "print numpy.mean(d[50:,:],0)\n",
    "\n",
    "plt.scatter(d[:,0],d[:,1],c=cl,cmap=matplotlib.cm.hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some classification methods. \n",
    "\n",
    "####Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adapted from http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#example-neighbors-plot-classification-py\n",
    "\n",
    "n_neighbors = 30\n",
    "\n",
    " # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\n",
    "clf.fit(d, cl)\n",
    "\n",
    "def plot_cls_with_decision_surface(d,cl,clf,h = .25 ):\n",
    "    \"\"\"Plot the decision boundary. For that, we will assign a color to each\n",
    "    point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    h= step size in the grid\n",
    "    \"\"\"\n",
    "    x_min, x_max = d[:, 0].min() - 1, d[:, 0].max() + 1\n",
    "    y_min, y_max = d[:, 1].min() - 1, d[:, 1].max() + 1\n",
    "    xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, h),\n",
    "                         numpy.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(numpy.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(d[:, 0], d[:, 1], c=cl, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "plot_cls_with_decision_surface(d,cl,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Change the number of nearest neighbors and see how it changes the surface.\n",
    "\n",
    "Now let's write a function to perform cross-validation and compute prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def classify(d,cl,clf,cv):\n",
    "    pred=numpy.zeros(n)\n",
    "    for train,test in cv:\n",
    "        clf.fit(d[train,:],cl[train])\n",
    "        pred[test]=clf.predict(d[test,:])\n",
    "    return sklearn.metrics.accuracy_score(cl,pred),sklearn.metrics.confusion_matrix(cl,pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=sklearn.neighbors.KNeighborsClassifier(n_neighbors, weights='uniform')\n",
    "# use stratified k-fold crossvalidation, which keeps the proportion of classes roughly\n",
    "# equal across folds\n",
    "cv=sklearn.cross_validation.StratifiedKFold(cl, 8)\n",
    "acc,confusion=classify(d,cl,clf,cv)\n",
    "print acc\n",
    "print confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:  Loop through different levels of n_neighbors (from 1 to 30) and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_knn=numpy.zeros(30)\n",
    "for i in range(1,31):\n",
    "    clf=sklearn.neighbors.KNeighborsClassifier(i, weights='uniform')\n",
    "    accuracy_knn[i-1],_=classify(d,cl,clf,cv)\n",
    "plt.plot(range(1,31),accuracy_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a loop that does this using 100 different randomly generated datasets, and plot the mean across datasets.  This will take a couple of minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_knn=numpy.zeros((100,30))\n",
    "for x in range(100):\n",
    "    ds_cl,ds_x=make_class_data(multiplier=[1.1,1.1],N=n)\n",
    "    ds_cv=sklearn.cross_validation.StratifiedKFold(ds_cl, 8)\n",
    "    for i in range(1,31):\n",
    "        clf=sklearn.neighbors.KNeighborsClassifier(i, weights='uniform')\n",
    "        accuracy_knn[x,i-1],_=classify(ds_x,ds_cl,clf,ds_cv)\n",
    "plt.plot(range(1,31),numpy.mean(accuracy_knn,0))\n",
    "plt.xlabel('number of nearest neighbors')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=sklearn.lda.LDA()\n",
    "cv=sklearn.cross_validation.LeaveOneOut(n)\n",
    "acc,confusion=classify(d,cl,clf,cv)\n",
    "print acc\n",
    "print confusion\n",
    "plot_cls_with_decision_surface(d,cl,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=sklearn.linear_model.LogisticRegression(C=0.5)\n",
    "acc,confusion=classify(d,cl,clf,cv)\n",
    "print acc\n",
    "print confusion\n",
    "plot_cls_with_decision_surface(d,cl,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=sklearn.svm.SVC(kernel='linear')\n",
    "acc,confusion=classify(d,cl,clf,cv)\n",
    "print acc\n",
    "print confusion\n",
    "plot_cls_with_decision_surface(d,cl,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Implement the example above using a nonlinear SVM with a radial basis kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf=sklearn.svm.SVC(kernel='rbf')\n",
    "acc,confusion=classify(d,cl,clf,cv)\n",
    "print acc\n",
    "print confusion\n",
    "plot_cls_with_decision_surface(d,cl,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Try the RBF SVC using a range of values for the gamma parameter.  \n",
    "NOTE: For real data analysis, you cannot determine the best value of gamma this way, because you would be peeking at the test data which will make your results overly optimistic.  Instead, you would need to use nested cross-validation loops; we will come to this later"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gammavals=numpy.arange(0.0,0.2,0.01)\n",
    "accuracy_rbf=numpy.zeros(len(gammavals))\n",
    "for i in range(len(gammavals)):\n",
    "    clf=sklearn.svm.SVC(kernel='rbf',gamma=gammavals[i])\n",
    "    accuracy_rbf[i],_=classify(d,cl,clf,cv)\n",
    "plt.plot(gammavals,accuracy_rbf)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the boundary for the classifier with the best performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxgamma=gammavals[numpy.where(accuracy_rbf==numpy.max(accuracy_rbf))]\n",
    "if len(maxgamma)>1:\n",
    "    maxgamma=maxgamma[0]\n",
    "print 'Best gamma:', maxgamma\n",
    "clf=sklearn.svm.SVC(kernel='rbf',gamma=maxgamma)\n",
    "acc,_=classify(d,cl,clf,cv)\n",
    "print 'Accuracy:',acc\n",
    "plot_cls_with_decision_surface(d,cl,clf)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}