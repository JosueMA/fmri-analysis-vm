{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the concepts underlying design efficiency.\n",
    "\n",
    "In order to examine the factors that affect efficiency, we need to be able to generate experimental designs that vary in their timing and correlation between regressors. Let's first create a function that can generate such designs for us."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "%matplotlib inline\n",
    "from mkdesign import create_design_singlecondition\n",
    "import matplotlib.pyplot as plt\n",
    "from spm_hrf import spm_hrf\n",
    "from nipy.modalities.fmri.hemodynamic_models import spm_hrf,compute_regressor\n",
    "\n",
    "# the \"blockiness\" argument controls how block-y the design is\n",
    "# from 1( pure block) to 0 (pure random)\n",
    "d,design=create_design_singlecondition(blockiness=0.9)\n",
    "plt.axis([0,400,-0.2,1.2])\n",
    "plt.plot(d)\n",
    "\n",
    "tr=1.0\n",
    "\n",
    "\n",
    "regressor,_=compute_regressor(design,'spm',numpy.arange(0,len(d)))\n",
    "plt.plot(regressor,color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our design, let's generate some synthetic data.  We will generate AR1 noise to add to the data; this is not a perfect model of the autocorrelation in fMRI, but it's at least a start towards realistic noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_process import arma_generate_sample\n",
    "\n",
    "ar1_noise=arma_generate_sample([1,0.3],[1,0.],len(regressor))\n",
    "beta=4\n",
    "y=regressor.T*beta + ar1_noise\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the general linear model to these data. We will ignore serial autocorrelation for now."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=numpy.vstack((regressor.T,numpy.ones(y.shape))).T\n",
    "plt.imshow(X,interpolation='nearest',cmap='gray')\n",
    "plt.axis('auto')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta_hat=numpy.linalg.inv(X.T.dot(X)).dot(X.T).dot(y.T)\n",
    "y_est=X.dot(beta_hat)\n",
    "plt.plot(y.T,color='blue')\n",
    "plt.plot(y_est,color='red',linewidth=2)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function to repeatedly generate data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_and_fit(X,beta=[4,10],c=[1,0]):\n",
    "    assert len(c)==X.shape[1]\n",
    "    ar1_noise=arma_generate_sample([1,0.3],[1,0.],X.shape[0])\n",
    "    y=X.dot(beta) + ar1_noise.T\n",
    "    beta_hat=numpy.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    resid=y - X.dot(beta_hat)\n",
    "    c=numpy.array(c)\n",
    "    sigma2_hat=c.dot(numpy.linalg.inv(X.T.dot(X))).dot(c)*(resid.dot(resid)/(X.shape[0] - X.shape[1] - 1))\n",
    "    return beta_hat,sigma2_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a simulation that creates datasets with varying levels of blockiness, runs the previous function 1000 times for each level, and computes the variability of the estimates"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nruns=1000\n",
    "blockiness_vals=numpy.arange(0,1.1,0.1)\n",
    "stdvals_blockiness=numpy.zeros((2,len(blockiness_vals)))\n",
    "meansigmahat_blockiness=numpy.zeros(len(blockiness_vals))\n",
    "\n",
    "for b in range(len(blockiness_vals)):\n",
    "    beta_hats=numpy.zeros((2,nruns))\n",
    "    sigma_hats=numpy.zeros(nruns)\n",
    "    \n",
    "    for i in range(nruns):\n",
    "        d_sim,design_sim=create_design_singlecondition(blockiness=blockiness_vals[b])\n",
    "        regressor_sim,_=compute_regressor(design_sim,'spm',numpy.arange(0,len(d_sim)))\n",
    "        X=numpy.vstack((regressor_sim.T,numpy.ones(y.shape))).T\n",
    "        beta_hats[:,i],sigma_hats[i]=generate_and_fit(X)\n",
    "    stdvals_blockiness[:,b]=numpy.var(beta_hats,1)\n",
    "    meansigmahat_blockiness[b]=numpy.mean(sigma_hats)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(blockiness_vals,1./stdvals_blockiness[0,:])\n",
    "plt.xlabel('blockiness')\n",
    "plt.ylabel('efficiency')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(blockiness_vals,1./meansigmahat_blockiness)\n",
    "plt.xlabel('blockiness')\n",
    "plt.ylabel('efficiency')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a similar simulation looking at the effects of varying block length between 10 seconds and 120 seconds (in steps of 10)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nruns=1000\n",
    "blocklenvals=numpy.arange(10,120,5)\n",
    "stdvals_blocklen=numpy.zeros((2,len(blocklenvals)))\n",
    "meansigmahat_blocklen=numpy.zeros(len(blocklenvals))\n",
    "\n",
    "for b in range(len(blocklenvals)):\n",
    "    beta_hats=numpy.zeros((2,nruns))\n",
    "    sigma_hats=numpy.zeros(nruns)\n",
    "    for i in range(nruns):\n",
    "        d_sim,design_sim=create_design_singlecondition(blocklength=blocklenvals[b],blockiness=1.)\n",
    "        regressor_sim,_=compute_regressor(design_sim,'spm',numpy.arange(0,len(d_sim)))\n",
    "        X=numpy.vstack((regressor_sim.T,numpy.ones(y.shape))).T\n",
    "        beta_hats[:,i],sigma_hats[i]=generate_and_fit(X)\n",
    "    stdvals_blocklen[:,b]=numpy.std(beta_hats,1)\n",
    "    meansigmahat_blocklen[b]=numpy.mean(sigma_hats)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(blocklenvals,1./stdvals_blocklen[0,:])\n",
    "plt.xlabel('block length')\n",
    "plt.ylabel('efficiency')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(blocklenvals,1./meansigmahat_blocklen)\n",
    "plt.xlabel('block length')\n",
    "plt.ylabel('efficiency')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the effects of correlation between regressors. We first need to create a function to generate a design with two conditions where we can control the correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mkdesign import create_design_twocondition\n",
    "\n",
    "d,des1,des2=create_design_twocondition(correlation=1.0)\n",
    "regressor1,_=compute_regressor(des1,'spm',numpy.arange(0,d.shape[0]))\n",
    "regressor2,_=compute_regressor(des2,'spm',numpy.arange(0,d.shape[0]))\n",
    "\n",
    "X=numpy.vstack((regressor1.T,regressor2.T,numpy.ones(y.shape))).T"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nruns=500\n",
    "corr_vals=numpy.arange(0,1.1,0.1)\n",
    "stdvals_corr=numpy.zeros((3,len(corr_vals)))\n",
    "corrvals=numpy.zeros(len(corr_vals))\n",
    "meansigmahat_corr=numpy.zeros(len(corr_vals))\n",
    "for b in range(len(corr_vals)):\n",
    "    beta_hats=numpy.zeros((3,nruns))\n",
    "    sigma_hats=numpy.zeros(nruns)\n",
    "    corrs=numpy.zeros(nruns)\n",
    "    for i in range(nruns):\n",
    "        d_sim,des1_sim,des2_sim=create_design_twocondition(correlation=corr_vals[b])\n",
    "        regressor1_sim,_=compute_regressor(des1_sim,'spm',numpy.arange(0,d_sim.shape[0]))\n",
    "        regressor2_sim,_=compute_regressor(des2_sim,'spm',numpy.arange(0,d_sim.shape[0]))\n",
    "        X=numpy.vstack((regressor1_sim.T,regressor2_sim.T,numpy.ones(y.shape))).T\n",
    "        # use contrast of first regressor\n",
    "        beta_hats[:,i],sigma_hats[i]=generate_and_fit(X,[3,5,10],c=[1,0,0])\n",
    "        corrs[i]=numpy.corrcoef(X.T)[0,1]\n",
    "    stdvals_corr[:,b]=numpy.std(beta_hats,1)\n",
    "    corrvals[b]=numpy.mean(corrs)\n",
    "    meansigmahat_corr[b]=numpy.mean(sigma_hats)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(corr_vals,1./stdvals_corr[0,:])\n",
    "plt.xlabel('mean correlation between regressors')\n",
    "plt.ylabel('efficiency')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(corr_vals,1./meansigmahat_corr)\n",
    "plt.xlabel('mean correlation between regressors')\n",
    "plt.ylabel('efficiency')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at efficiency of estimation of the shape of the HRF, rather than detection of the activation effect.  This requires that we use a finite impulse response (FIR) model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d,design=create_design_singlecondition(blockiness=0.0)\n",
    "regressor,_=compute_regressor(design,'fir',numpy.arange(0,len(d)),fir_delays=numpy.arange(0,16))\n",
    "plt.imshow(regressor[:50,:],interpolation='nearest',cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's simulate the FIR model, and estimate the variance of the fits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print fir_betas.shape\n",
    "print regressor.shape\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_and_fit_trace(X,beta=None,signal=2):\n",
    "    \"\"\"return trace of covariance matrix for all but last regressor (which is the mean)\"\"\"\n",
    "    if not beta:\n",
    "        beta=numpy.hstack((signal*spm_hrf(1,oversampling=1)[:X.shape[1]-1],10))\n",
    "    ar1_noise=arma_generate_sample([1,0.3],[1,0.],X.shape[0])\n",
    "    y=X.dot(beta) + ar1_noise.T\n",
    "    beta_hat=numpy.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    resid=y - X.dot(beta_hat)\n",
    "    sigma2_hat=numpy.trace(numpy.linalg.inv(X[:,:-1].T.dot(X[:,:-1])))*(resid.dot(resid)/(X.shape[0] - X.shape[1] - 1))\n",
    "    return beta_hat,sigma2_hat\n",
    "    \n",
    "    \n",
    "nruns=100\n",
    "blockiness_vals=numpy.arange(0,1.1,0.1)\n",
    "stdvals_blockiness=numpy.zeros((17,len(blockiness_vals)))\n",
    "betavals_blockiness=numpy.zeros((17,len(blockiness_vals)))\n",
    "meanstd_blockiness=numpy.zeros(len(blockiness_vals))\n",
    "\n",
    "fir_betas=numpy.hstack((10*spm_hrf(1,oversampling=1)[:16],10))\n",
    "\n",
    "for b in range(len(blockiness_vals)):\n",
    "    beta_hats=numpy.zeros((17,nruns))\n",
    "    sigma_hats=numpy.zeros(nruns)\n",
    "    for i in range(nruns):\n",
    "        d_sim,design_sim=create_design_singlecondition(blockiness=blockiness_vals[b])\n",
    "        regressor_sim,_=compute_regressor(design_sim,'fir',\n",
    "                            numpy.arange(0,len(d_sim)),fir_delays=numpy.arange(0,16))\n",
    "        X=numpy.vstack((regressor_sim.T,numpy.ones(regressor_sim.shape[0]))).T\n",
    "        contrast=numpy.ones(X.shape[1])\n",
    "        contrast[-1]=0\n",
    "        beta_hats[:,i],sigma_hats[i]=generate_and_fit_trace(X)\n",
    "        \n",
    "    stdvals_blockiness[:,b]=numpy.std(beta_hats,1)\n",
    "    betavals_blockiness[:,b]=numpy.mean(beta_hats,1)\n",
    "    meanstd_blockiness[b]=numpy.mean(sigma_hats)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(blockiness_vals,1./meanstd_blockiness)\n",
    "plt.xlabel('blockiness')\n",
    "plt.ylabel('efficiency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: write a function to generate random designs, and then do this a large number of times, each time estimating the efficiency.  Then plot the histogram of efficiencies. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}